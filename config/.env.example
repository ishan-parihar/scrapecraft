# API Keys
OPENROUTER_API_KEY=your-openrouter-api-key-here
# SCRAPEGRAPH_API_KEY=your-scrapegraph-api-key-here  # Only required when not using local scraping

# Local scraping configuration
USE_LOCAL_SCRAPING=true

# LLM Provider Selection
LLM_PROVIDER=openrouter  # Options: "openrouter", "openai", "custom"

# OpenRouter Configuration (default)
OPENROUTER_MODEL=moonshotai/kimi-k2

# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4-turbo

# Custom LLM Provider Configuration
CUSTOM_LLM_ENABLED=false
CUSTOM_LLM_BASE_URL=https://api.your-provider.com/v1
CUSTOM_LLM_API_KEY=your-api-key-here
CUSTOM_LLM_MODEL=your-custom-model

# Custom LLM Provider Type (auto-detection if not specified)
# Options: "ollama", "localai", "vllm", "openai-compatible", "custom"
CUSTOM_LLM_PROVIDER_TYPE=openai-compatible

# Custom LLM Connection Settings
CUSTOM_LLM_TIMEOUT=120
CUSTOM_LLM_MAX_RETRIES=3
CUSTOM_LLM_RETRY_DELAY=1.0
CUSTOM_LLM_CONNECTION_POOL_SIZE=10
CUSTOM_LLM_KEEP_ALIVE=true

# Custom LLM Performance Settings (inherits from global settings if None)
# CUSTOM_LLM_TEMPERATURE=0.1
# CUSTOM_LLM_MAX_TOKENS=4000
# CUSTOM_LLM_TOP_P=0.9
# CUSTOM_LLM_FREQUENCY_PENALTY=0.0
# CUSTOM_LLM_PRESENCE_PENALTY=0.0
# CUSTOM_LLM_SEED=42

# Custom LLM Streaming Settings
CUSTOM_LLM_STREAMING=true
CUSTOM_LLM_STREAM_BUFFER_SIZE=1024
CUSTOM_LLM_STREAM_TIMEOUT=30

# Custom LLM Model Validation
CUSTOM_LLM_VALIDATE_MODEL=true
CUSTOM_LLM_AUTO_DISCOVER_MODELS=false
# CUSTOM_LLM_MODEL_LIST_ENDPOINT=/v1/models
# CUSTOM_LLM_SUPPORTED_MODELS=["llama3.2:instruct", "codellama:7b", "mistral:7b"]

# Custom LLM Security Settings
CUSTOM_LLM_VERIFY_SSL=true
CUSTOM_LLM_ALLOW_SELF_SIGNED=false
# CUSTOM_LLM_API_KEY_HEADER=X-API-Key
# CUSTOM_LLM_API_KEY_PREFIX=Bearer 

# Custom LLM Caching Settings
CUSTOM_LLM_ENABLE_CACHE=false
CUSTOM_LLM_CACHE_TTL=3600
CUSTOM_LLM_CACHE_SIZE=1000

# Custom LLM Advanced Features
CUSTOM_LLM_ENABLE_FALLBACK=false
# CUSTOM_LLM_FALLBACK_PROVIDERS=["http://localhost:8080/v1", "http://localhost:8000/v1"]
CUSTOM_LLM_HEALTH_CHECK_INTERVAL=300
CUSTOM_LLM_PERFORMANCE_MONITORING=true

# Provider-Specific Default Settings
OLLAMA_DEFAULT_MODEL=llama3.2:instruct
OLLAMA_PULL_MODELS=false
# OLLAMA_GPU_LAYERS=35

LOCALAI_DEFAULT_MODEL=ggml-gpt4all-j
LOCALAI_CONTEXT_SIZE=2048
LOCALAI_THREADS=4

VLLM_DEFAULT_MODEL=your-model
VLLM_TENSOR_PARALLEL_SIZE=1
# VLLM_MAX_MODEL_LENGTH=4096

# ====== CUSTOM PROVIDER SETUP EXAMPLES ======

# Example 1: Ollama (Local Models)
# LLM_PROVIDER=custom
# CUSTOM_LLM_ENABLED=true
# CUSTOM_LLM_BASE_URL=http://localhost:11434/v1
# CUSTOM_LLM_PROVIDER_TYPE=ollama
# CUSTOM_LLM_MODEL=llama3.2:instruct
# CUSTOM_LLM_API_KEY=ollama  # Ollama doesn't require real API keys
#
# Popular Ollama Models:
# - llama3.2:instruct (3B/8B parameters)
# - codellama:7b (Code generation)
# - mistral:7b (General purpose)
# - qwen2.5:7b (Multilingual)
# - deepseek-coder:6.7b (Code specialist)

# Example 2: LocalAI (Self-hosted models)
# LLM_PROVIDER=custom
# CUSTOM_LLM_ENABLED=true
# CUSTOM_LLM_BASE_URL=http://localhost:8080/v1
# CUSTOM_LLM_PROVIDER_TYPE=localai
# CUSTOM_LLM_MODEL=ggml-gpt4all-j
# CUSTOM_LLM_API_KEY=your-localai-key
#
# LocalAI supports:
# - GGUF models (quantized)
# - PyTorch models
# - Custom fine-tuned models
# - Multiple model instances

# Example 3: vLLM (High-performance inference)
# LLM_PROVIDER=custom
# CUSTOM_LLM_ENABLED=true
# CUSTOM_LLM_BASE_URL=http://localhost:8000/v1
# CUSTOM_LLM_PROVIDER_TYPE=vllm
# CUSTOM_LLM_MODEL=your-model-name
# CUSTOM_LLM_API_KEY=your-vllm-key
#
# vLLM optimized for:
# - Large model inference
# - High throughput
# - GPU acceleration
# - Batch processing

# Example 4: Custom Cloud Provider (OpenAI-compatible)
# LLM_PROVIDER=custom
# CUSTOM_LLM_ENABLED=true
# CUSTOM_LLM_BASE_URL=https://your-inference-api.com/v1
# CUSTOM_LLM_PROVIDER_TYPE=openai-compatible
# CUSTOM_LLM_MODEL=your-cloud-model
# CUSTOM_LLM_API_KEY=your-cloud-api-key
# CUSTOM_LLM_VERIFY_SSL=true
#
# Compatible with:
# - Together AI
# - Groq
# - Anyscale
# - Fireworks AI
# - Custom OpenAI-compatible endpoints

# LLM Settings
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=4000

# Legacy LLM Configuration for Local ScrapeGraphAI (deprecated, use above instead)
# Option 1: OpenAI (default)
# LOCAL_LLM_MODEL=gpt-3.5-turbo

# Option 2: Ollama (uncomment to use)
# USE_OLLAMA=true
# OLLAMA_MODEL=llama3.2
# OLLAMA_BASE_URL=http://localhost:11434

# Option 3: Other OpenAI-compatible APIs
# LOCAL_LLM_API_KEY=your-api-key
# LOCAL_LLM_MODEL=your-model-name
# LOCAL_LLM_BASE_URL=https://api.your-provider.com/v1

# Database
DATABASE_URL=postgresql://user:pass@db:5432/scrapecraft

# Security
JWT_SECRET=your-super-secret-jwt-secret-key-here

# CORS - Add your frontend URLs
CORS_ORIGINS=["http://localhost:3000", "http://localhost:80"]

# ScrapeGraphAI Settings
SCRAPEGRAPHAI_HEADLESS=true
SCRAPEGRAPHAI_VERBOSE=false